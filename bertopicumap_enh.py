# -*- coding: utf-8 -*-
"""BertopicUMAP_enh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WHBXbqpeb3xA-0lIJXCZvf7KNfw3yjvJ

# Packages
"""

!pip install torch
# For a specific CUDA version (e.g., CUDA 11.3)
!pip install torch==2.3.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html
# Reinstall sentence-transformers and bertopic
!pip install sentence-transformers
!pip install bertopic
import pandas as pd
from bertopic import BERTopic
import re
import gensim

!pip install tensorflow

from nltk.corpus import stopwords

# Make sure to download the stopwords set if you haven't already
import nltk
nltk.download('stopwords')

stop_words = stopwords.words('english')
stop_words.extend([
    'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along',
    'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another',
    'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became',
    'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside',
    'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co',
    'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during',
    'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every',
    'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five',
    'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go',
    'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers',
    'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest',
    'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many',
    'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must',
    'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none',
    'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or',
    'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please',
    'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should',
    'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime',
    'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them',
    'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these',
    'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to',
    'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us',
    'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter',
    'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever',
    'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours',
    'yourself', 'yourselves'
]
)

# Load the dataset
df_og = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')
df = df_og.sample(frac=0.4, random_state=1)

"""# Data"""

df

"""# Data preparation"""

def sent_to_words(sentences):
    for sent in sentences:
        sent = re.sub('\S*@\S*\s?', '', sent)  # remove emails
        sent = re.sub('\s+', ' ', sent)  # remove newline chars
        sent = re.sub("\'", "", sent)  # remove single quotes
        tokens = gensim.utils.simple_preprocess(str(sent), deacc=True)  # Tokenize and remove punctuations
        filtered_tokens = [word for word in tokens if word not in stop_words]  # Filter out the stop words
        yield filtered_tokens

# Apply the function to each row of the content column
data = df['content'].tolist()  # Convert the 'content' column to a list
data_words = list(sent_to_words(data))  # Generate cleaned text

# Convert lists of words back into strings, as BERTopic uses raw text
documents = [' '.join(doc) for doc in data_words]

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt')

filtered_text = []
lemmatizer = WordNetLemmatizer()

for w in documents:
  filtered_text.append(lemmatizer.lemmatize(w))
print(filtered_text[:1])

"""# Base BERTopic Model"""

from umap import UMAP
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from bertopic.vectorizers import ClassTfidfTransformer

from transformers.pipelines import pipeline

embedding_model = pipeline("feature-extraction", model="distilbert-base-cased")

# Step 1 - Extract embeddings
#embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Step 2 - Reduce dimensionality
umap_model = UMAP(n_neighbors=15, n_components=7, min_dist=0.0, metric='cosine')

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Step 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words="english")

# Step 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer()

# Step 6 - (Optional) Fine-tune topic representations with
# a `bertopic.representation` model
representation_model = KeyBERTInspired()

# All steps together
topic_model = BERTopic(
  embedding_model=embedding_model,          # Step 1 - Extract embeddings
  umap_model=umap_model,                    # Step 2 - Reduce dimensionality
  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings
  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics
  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words
  #representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations
)

topics,probs= topic_model.fit_transform(filtered_text)

topic_model.get_topic_info()

"""# Coherence Score Evaluation"""

!pip install gensim
import gensim.corpora as corpora
from gensim.models.coherencemodel import CoherenceModel # Import CoherenceModel

documents_pd = pd.DataFrame({"Document": filtered_text,
                          "ID": range(len(filtered_text)),
                          "Topic": topics})
documents_per_topic = documents_pd.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})
cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)
# Extract vectorizer and analyzer from BERTopic
vectorizer = topic_model.vectorizer_model
analyzer = vectorizer.build_analyzer()
words = vectorizer.get_feature_names_out()
tokens = [analyzer(doc) for doc in cleaned_docs]
dictionary = corpora.Dictionary(tokens)
corpus = [dictionary.doc2bow(token) for token in tokens]
topic_words = [[words for words, _ in topic_model.get_topic(topic)]
               for topic in range(9)]

# Evaluate
coherence_model = CoherenceModel(topics=topic_words,
                                 texts=tokens,
                                 corpus=corpus,
                                 dictionary=dictionary,
                                 coherence='c_v')
coherence = coherence_model.get_coherence()
coherence

"""# New model"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from umap import UMAP
from sentence_transformers import SentenceTransformer
from tensorflow.keras import layers, models, optimizers
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Initialize the embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(filtered_text)  # 'embeddings' will be your input to UMAP and Autoencoder

best_silhouette_score = -1
best_num_clusters = 0
# Step 1: Apply UMAP
umap_model = UMAP(n_neighbors=15, n_components=11, min_dist=0.0, metric='cosine')
umap_embeddings = umap_model.fit_transform(embeddings)
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(umap_embeddings)
    silhouette_avg = silhouette_score(umap_embeddings, cluster_labels)
    if silhouette_avg > best_silhouette_score:
        best_silhouette_score = silhouette_avg
        best_num_clusters = n_clusters

print(f"Best number of clusters: {best_num_clusters} with Silhouette Score: {best_silhouette_score}")

# List to store results
results = []
# Iterate over different numbers of dimensions (2 to 9)
for n_components in range(2, 10):
    print(f"Evaluating UMAP with {n_components} components...")

    # Step 1: Apply UMAP
    umap_model = UMAP(n_neighbors=15, n_components=n_components, min_dist=0.0, metric='cosine')
    umap_embeddings = umap_model.fit_transform(embeddings)

    # Step 2: Apply Clustering Algorithm (e.g., KMeans)
    num_clusters = 8  # Set the number of clusters (adjust as necessary)
    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(umap_embeddings)

    # Step 3: Calculate Silhouette Score
    silhouette_avg = silhouette_score(umap_embeddings, cluster_labels)
    print(f"Silhouette Score for {n_components} components: {silhouette_avg}")


    # Store results
    results.append({
        'n_components': n_components,
        'silhouette_score': silhouette_avg,
        #'reconstruction_error': reconstruction_error
    })

# After looping, combine results using a weighted metric or for comparison

embeddings.shape[1]

def build_and_train_autoencoder(input_dim, encoding_dim, data):
    # Define the autoencoder architecture
    input_layer = layers.Input(shape=(input_dim,))
    # Encoder with one hidden layer
    x = layers.Dense(200, activation='relu')(input_layer)
    encoder = layers.Dense(encoding_dim, activation='relu')(x)

    # Decoder with one hidden layer
    x = layers.Dense(200, activation='sigmoid')(encoder)
    decoder = layers.Dense(input_dim, activation='sigmoid')(x)
    # Create and compile the autoencoder
    autoencoder = models.Model(inputs=input_layer, outputs=decoder)
    autoencoder.compile(optimizer='adam', loss='mean_squared_error')

    # Normalize data
    scaler = MinMaxScaler()
    data_normalized = scaler.fit_transform(data)

    # Train-Test Split
    x_train, x_test = train_test_split(data_normalized, test_size=0.2, random_state=42)

    # Train Autoencoder
    autoencoder.fit(x_train, x_train,
                    epochs=25,
                    shuffle=True,
                    validation_data=(x_test, x_test))

    # Calculate Reconstruction Error
    reconstructed_data = autoencoder.predict(data_normalized)
    mse = mean_squared_error(data_normalized, reconstructed_data)

    return reconstruction_error

from sklearn.metrics import mean_squared_error

# Example usage
result2 = []
for n_components in range(2, 10):
    reconstruction_error = build_and_train_autoencoder(embeddings.shape[1], n_components, embeddings)
    print(f"Reconstruction Error for {n_components} components: {reconstruction_error}")

    # Store results
    result2.append({
        'n_components': n_components,
        'reconstruction_error': reconstruction_error
    })

result2

# Step 1 - Extract embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Step 2 - Reduce dimensionality
umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Step 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words="english")

# Step 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer()

# Step 6 - (Optional) Fine-tune topic representations with
# a `bertopic.representation` model
representation_model = KeyBERTInspired()

# All steps together
topic_model = BERTopic(
  embedding_model=embedding_model,          # Step 1 - Extract embeddings
  umap_model=umap_model,                    # Step 2 - Reduce dimensionality
  hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings
  vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics
  ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words
  #representation_model=representation_model # Step 6 - (Optional) Fine-tune topic represenations
)